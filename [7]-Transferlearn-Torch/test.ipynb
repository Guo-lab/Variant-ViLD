{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "NO Detection\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# https://haochen23.github.io/2020/04/object-detection-faster-rcnn.html#.Y0VLb8pBxJY\n",
    "\n",
    "FasterRCNNtest = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = FasterRCNNtest.roi_heads.box_predictor.cls_score.in_features\n",
    "FasterRCNNtest.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes=8)\n",
    "\n",
    "torch.save(FasterRCNNtest.state_dict(), f\"{'kitti-model'}/model-origin.pth\")\n",
    "FasterRCNNtest.load_state_dict(torch.load('./kitti-model/model4.pth'))\n",
    "\n",
    "FasterRCNNtest.eval()\n",
    "\n",
    "#FasterRCNN.eval()\n",
    "\n",
    "CLASS_NAMES = ['Car', 'Truck', 'Cyclist', 'Tram', 'Person_sitting', 'Misc', 'Van', 'Pedestrian']\n",
    "\n",
    "\n",
    "def get_prediction(img_path, confidence):\n",
    "  \"\"\"\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - confidence - threshold value for prediction score\n",
    "  \"\"\"\n",
    "  img = Image.open(img_path)\n",
    "  transform = T.Compose([T.ToTensor()])\n",
    "  img = transform(img)\n",
    "  pred = FasterRCNNtest([img]) # FasterRCNNtest\n",
    "  \n",
    "  print(pred)\n",
    "  if pred[0]['scores'].detach().numpy().size == 0:\n",
    "      return None, None\n",
    "  pred_class = [CLASS_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "\n",
    "  for x in pred_score:\n",
    "    if x > confidence:\n",
    "      print(pred_score.index(x))\n",
    "    if pred_score.index(x) == 0:\n",
    "      return [pred_boxes[0]], [pred_class[0]] \n",
    "    \n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x>confidence][-1]\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  return pred_boxes, pred_class\n",
    "\n",
    "\n",
    "def detect_object(img_path, confidence=0.5, rect_th=2, text_size=0.5, text_th=2):\n",
    "  \"\"\"\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - confidence - threshold value for prediction score\n",
    "      - rect_th - thickness of bounding box\n",
    "      - text_size - size of the class label text\n",
    "      - text_th - thichness of the text\n",
    "  \"\"\"\n",
    "  boxes, pred_cls = get_prediction(img_path, confidence)\n",
    "  if boxes == None and pred_cls == None:\n",
    "    print(\"NO Detection\")\n",
    "    return None\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "  for i in range(len(boxes)):\n",
    "    aa, bb = boxes[i][0][0], boxes[i][0][1]\n",
    "    a, b = boxes[i][1][0], boxes[i][1][1]\n",
    "    cv2.rectangle(img, (int(aa), int(bb)), (int(a), int(b)), \n",
    "                  color=(0, 255, 0), thickness=rect_th)\n",
    "    cv2.putText(img, pred_cls[i], (int(aa), int(bb)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                text_size, (0,255,0),thickness=text_th)\n",
    "  \n",
    "  plt.figure(figsize=(20,30))\n",
    "  plt.imshow(img)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()\n",
    "\n",
    "detect_object('./kitti-dataset/Kitti/raw/testing/image_2/000423.png', confidence=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets==7.6\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "for item in tqdm([0, 1, 2, 3, 4]):\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,1,1,1],[1,1,1,1]])\n",
    "a = a[:1]\n",
    "b = np.array([1,1,1,1,1])\n",
    "b = b[:4]\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tflearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6188715f2b82f6c50c42d987f8df2d3ad6eac92c9a4f79f193fe0ffa7a2662c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
