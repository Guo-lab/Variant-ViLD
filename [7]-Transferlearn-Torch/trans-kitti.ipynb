{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete the Transfer Learn on KITTI with pytorch\n",
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/103862272\n",
    "# https://gauenk.github.io/docs/how_faster_rcnn_accepts_various_input_image_sizes.pdf\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]) \n",
    "\n",
    "# https://github.com/pytorch/vision/issues/215\n",
    "def target_transform(target):\n",
    "    pass\n",
    "\n",
    "# https://github.com/xiaowei305/perception/blob/9965418a43cc04dc8ca78e3b1324704501c257f0/ch2/densebox/train.py\n",
    "import numpy as np\n",
    "def kitti_collate_fn(batch_data):\n",
    "    class_names = ['Car', 'Truck', 'Cyclist', 'Tram', 'Person_sitting', 'Misc', 'Van', 'Pedestrian']\n",
    "\n",
    "    unit_image, unit_target = [], []\n",
    "    for img, target in batch_data:\n",
    "        c, h, w = img.shape\n",
    "        boxes = np.array([x[\"bbox\"] for x in target if x['type'] != \"DontCare\"])\n",
    "        classes = np.array([class_names.index(x['type']) for x in target if x['type'] != \"DontCare\"])\n",
    "        #if len(boxes) <= max_boxes:\n",
    "        #    boxes = np.pad(boxes, ((0, max_boxes - len(boxes)), (0, 0))) # https://blog.csdn.net/Tan_HandSome/article/details/80296827\n",
    "        #    classes = np.pad(classes, (0, max_boxes - len(classes)))\n",
    "        #else:\n",
    "        #    boxes = boxes[:11]\n",
    "        #    classes = classes[:11]\n",
    "            \n",
    "        boxes /= np.array([w, h, w, h])\n",
    "        boxes = torch.from_numpy(np.stack(boxes, axis=0))\n",
    "        classes = torch.from_numpy(np.stack(classes, axis=0))\n",
    "\n",
    "        target_ = {}\n",
    "        target_['boxes'] = boxes\n",
    "        target_['labels'] = classes\n",
    "         \n",
    "        unit_image += [img]  \n",
    "        unit_target.append(target_) \n",
    "    return unit_image, unit_target\n",
    "\n",
    "# https://pytorch.org/vision/stable/generated/torchvision.datasets.Kitti.html#torchvision.datasets.Kitti\n",
    "# https://pytorch.org/vision/master/_modules/torchvision/datasets/kitti.html\n",
    "train_dataset = torchvision.datasets.Kitti(\n",
    "    root='./kitti-dataset',\n",
    "    train=True, download=True, transform=transform, #target_transform=target_transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=kitti_collate_fn)\n",
    "\n",
    "for batch in trainloader:\n",
    "    #print(batch)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Kitti\n",
      "    Number of datapoints: 240\n",
      "    Root location: ./kitti-dataset\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(800, 800), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.Kitti(\n",
    "    root='./kitti-dataset-for-valid',\n",
    "    train=True, download=True, transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=8, shuffle=True, collate_fn=kitti_collate_fn)\n",
    "\n",
    "# testloader train=Flase, just not fetch labels\n",
    "for batch in testloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[0.5953, 0.4388, 0.2016, 0.0624],\n",
      "        [0.5888, 0.5681, 0.9583, 0.9076],\n",
      "        [0.9424, 0.0798, 0.0354, 0.2861],\n",
      "        [0.4587, 0.8284, 0.8997, 0.6154],\n",
      "        [0.7531, 0.8675, 0.5503, 0.1531],\n",
      "        [0.5151, 0.5926, 0.8113, 0.1086],\n",
      "        [0.0847, 0.2823, 0.6391, 0.6936],\n",
      "        [0.1617, 0.8279, 0.8987, 0.7467],\n",
      "        [0.0821, 0.2013, 0.0910, 0.4601],\n",
      "        [0.4938, 0.7785, 0.1791, 0.4087],\n",
      "        [0.6775, 0.8924, 0.9092, 0.0739]]), 'labels': tensor([76, 55, 68, 51, 63, 37, 35, 61, 42, 55, 68])}\n"
     ]
    }
   ],
   "source": [
    "# demo in pytorch document #!DONOT RUN THIS CELL\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "labels = torch.randint(1, 91, (4, 11)) # from 1 to 91, each box in each png match one label\n",
    "images = list(image for image in images)\n",
    "targets = []\n",
    "for i in range(len(images)):\n",
    "    d = {}\n",
    "    d['boxes'] = boxes[i]\n",
    "    d['labels'] = labels[i]\n",
    "    print(d)\n",
    "    break\n",
    "    targets.append(d)\n",
    "\n",
    "#len(images)  # 4\n",
    "#len(targets) # 4\n",
    "\n",
    "#output = model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFasterRCNN(\\n  (transform): GeneralizedRCNNTransform(\\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\\n  )\\n  (backbone): BackboneWithFPN(\\n    (body): IntermediateLayerGetter(\\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n      (relu): ReLU(inplace=True)\\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\\n      (layer1): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n            (1): FrozenBatchNorm2d(256, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n      (layer2): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n            (1): FrozenBatchNorm2d(512, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (3): Bottleneck(\\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n      (layer3): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (3): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (4): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (5): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n      (layer4): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n    )\\n    (fpn): FeaturePyramidNetwork(\\n      (inner_blocks): ModuleList(\\n        (0): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n        (1): Conv2dNormActivation(\\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n        (2): Conv2dNormActivation(\\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n        (3): Conv2dNormActivation(\\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n      )\\n      (layer_blocks): ModuleList(\\n        (0): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n        (1): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n        (2): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n        (3): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n      )\\n      (extra_blocks): LastLevelMaxPool()\\n    )\\n  )\\n  (rpn): RegionProposalNetwork(\\n    (anchor_generator): AnchorGenerator()\\n    (head): RPNHead(\\n      (conv): Sequential(\\n        (0): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n          (1): ReLU(inplace=True)\\n        )\\n      )\\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\\n    )\\n  )\\n  (roi_heads): RoIHeads(\\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\\n    (box_head): TwoMLPHead(\\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\\n    )\\n    (box_predictor): FastRCNNPredictor(\\n      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\\n      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\\n    )\\n  )\\n)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://pytorch.org/vision/master/models/faster_rcnn.html\n",
    "FasterRCNN = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "'''\n",
    "FasterRCNN(\n",
    "  (transform): GeneralizedRCNNTransform(\n",
    "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
    "  )\n",
    "  (backbone): BackboneWithFPN(\n",
    "    (body): IntermediateLayerGetter(\n",
    "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "      (layer1): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (layer2): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (3): Bottleneck(\n",
    "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (layer3): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (3): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (4): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (5): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (layer4): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (fpn): FeaturePyramidNetwork(\n",
    "      (inner_blocks): ModuleList(\n",
    "        (0): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): Conv2dNormActivation(\n",
    "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (2): Conv2dNormActivation(\n",
    "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (3): Conv2dNormActivation(\n",
    "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (layer_blocks): ModuleList(\n",
    "        (0): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "        (1): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "        (2): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "        (3): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (extra_blocks): LastLevelMaxPool()\n",
    "    )\n",
    "  )\n",
    "  (rpn): RegionProposalNetwork(\n",
    "    (anchor_generator): AnchorGenerator()\n",
    "    (head): RPNHead(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (1): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
    "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
    "    )\n",
    "  )\n",
    "  (roi_heads): RoIHeads(\n",
    "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
    "    (box_head): TwoMLPHead(\n",
    "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
    "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "    )\n",
    "    (box_predictor): FastRCNNPredictor(\n",
    "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
    "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model pre-trained\n",
    "in_features = FasterRCNN.roi_heads.box_predictor.cls_score.in_features\n",
    "FasterRCNN.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for images, labels in iter(trainloader):\n",
    "    #print(images)\n",
    "    print(len(images))\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://debuggercafe.com/custom-object-detection-using-pytorch-faster-rcnn/\n",
    "- https://debuggercafe.com/a-simple-pipeline-to-train-pytorch-faster-rcnn-object-detection-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin\n",
    "NUM_EPOCHS = 3\n",
    "BEST_MODEL_PATH = 'kitti-model/best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "optimizer = optim.SGD(FasterRCNN.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    FasterRCNN.train()\n",
    "    for images, labels in iter(trainloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = FasterRCNN(images, labels)\n",
    "        #{'loss_classifier': tensor(3.0745, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0106, grad_fn=<DivBackward0>), 'loss_objectness': tensor(2.0545, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.3918, dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
    "        \n",
    "        #loss = F.cross_entropy(outputs, labels)\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #test_error_count = 0.0\n",
    "    #FasterRCNN.eval()\n",
    "    # NO testloader\n",
    "    #for images, targets in iter(trainloader):\n",
    "        #boxes = targets['boxes']\n",
    "        #labels = targets['labels']\n",
    "        \n",
    "        #pred_boxes, predicted_labels, predicted_scores = FasterRCNN(images)\n",
    "        \n",
    "        #test_error_count += float(torch.sum(torch.abs(labels - predicted_labels.argmax(1))))\n",
    "    \n",
    "    #test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset))\n",
    "    #print('%d: %f' % (epoch, test_accuracy))\n",
    "    #if test_accuracy > best_accuracy:\n",
    "    #    torch.save(FasterRCNN.state_dict(), BEST_MODEL_PATH)\n",
    "    #    best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14d69e24c9d4d1aaa71afea34f40fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7d9bb2f1294d869ed4936e187db8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #0 TRAIN LOSS: 0.870 and VALID LOSS: 0.289\n",
      "\n",
      "EPOCH 2 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43a15dcc1af42a7a9665affe98dc20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02e9d55341a4dc29933d13095dcfb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #1 TRAIN LOSS: 0.286 and VALID LOSS: 0.314\n",
      "SAVING MODEL COMPLETE...\n",
      "\n",
      "\n",
      "EPOCH 3 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44447769bdfd4dcab34d82ea6a457934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b21fd13b016400f87a432fa6095a5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #2 TRAIN LOSS: 0.270 and VALID LOSS: 0.222\n",
      "\n",
      "EPOCH 4 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b03705f6ffa4f0894323543c8bbaf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da5bb4f5d04b69973c71e78d0e948c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #3 TRAIN LOSS: 0.231 and VALID LOSS: 0.203\n",
      "SAVING MODEL COMPLETE...\n",
      "\n",
      "\n",
      "EPOCH 5 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33176f7151c4a7b972fc1ab52422739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c30c78479b94ce3821001e7af0dc70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #4 TRAIN LOSS: 0.221 and VALID LOSS: 0.349\n",
      "\n",
      "EPOCH 6 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829cb38cd910469ea286552c47dd0e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33a1a0d053d4eeb8f0b83d0eaaeb483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #5 TRAIN LOSS: 0.238 and VALID LOSS: 0.203\n",
      "SAVING MODEL COMPLETE...\n",
      "\n",
      "\n",
      "EPOCH 7 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78caa374864c458581b422bb39e6df72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66ad669a2f848d088581f6e1fd5f456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #6 TRAIN LOSS: 0.208 and VALID LOSS: 0.188\n",
      "\n",
      "EPOCH 8 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482e682803eb4cf4a0ee11e4eb164d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88f4fe80c3249cea1e74e5ed3da39b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #7 TRAIN LOSS: 0.218 and VALID LOSS: 0.289\n",
      "SAVING MODEL COMPLETE...\n",
      "\n",
      "\n",
      "EPOCH 9 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da65aa4d16b44235a2d98d3e055d9f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b605310f2951442197877e3cc3471302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #8 TRAIN LOSS: 0.231 and VALID LOSS: 0.191\n",
      "\n",
      "EPOCH 10 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c907df1894c94e40b3bb1cc650f62f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN END ####\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6adc88563b4afc8812e7c2bb58ca8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "EPOCH #9 TRAIN LOSS: 0.208 and VALID LOSS: 0.163\n",
      "SAVING MODEL COMPLETE...\n",
      "\n",
      "\n",
      "EPOCH 11 of 300:\n",
      "#### TRAINING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd6be32bf654f21aa04cc8e755f4d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 300\n",
    "best_accuracy = 0.0\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    \n",
    "params = [p for p in FasterRCNN.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "train_loss_list, val_loss_list = [], [] \n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "min_loss = 0.15\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "  \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}:\")\n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()\n",
    "    \n",
    "    \n",
    "    #FasterRCNN.train()\n",
    "    print(\"#### TRAINING ####\")\n",
    "    prog_bar = tqdm(trainloader, total=len(trainloader))\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        #//print(images, targets)\n",
    "        \n",
    "        loss_dict = FasterRCNN(images, targets)\n",
    "        #{'loss_classifier': tensor(3.0745, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0106, grad_fn=<DivBackward0>), 'loss_objectness': tensor(2.0545, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.3918, dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
    "        #//print(loss_dict)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    print(\"#### TRAIN END ####\")\n",
    "    \n",
    "    \n",
    "    print(\"#### VALIDATING ####\")\n",
    "    prog_bar = tqdm(trainloader, total=len(trainloader))\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        \n",
    "        images, targets = data \n",
    "               \n",
    "        with torch.no_grad():\n",
    "            loss_dict = FasterRCNN(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        \n",
    "        val_itr += 1\n",
    "    print(\"#### VALID END ####\")\n",
    "    \n",
    "    print(f\"EPOCH #{epoch} TRAIN LOSS: {train_loss_hist.value:.3f} and VALID LOSS: {val_loss_hist.value:.3f}\")   \n",
    "    if train_loss_hist.value < min_loss:\n",
    "        min_loss = train_loss_hist.value\n",
    "        torch.save(FasterRCNN.state_dict(), f\"{'kitti-model'}/model_best.pth\")  \n",
    "              \n",
    "    if (epoch+1) % 2 == 0: # save model after every n epochs\n",
    "        torch.save(FasterRCNN.state_dict(), f\"{'kitti-model'}/model{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_56777/4202895266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mdetect_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./kitti-dataset/Kitti/raw/testing/image_2/000423.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_56777/4202895266.py\u001b[0m in \u001b[0;36mdetect_object\u001b[0;34m(img_path, confidence, rect_th, text_size, text_th)\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0;34m-\u001b[0m \u001b[0mtext_th\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthichness\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \"\"\"\n\u001b[0;32m---> 60\u001b[0;31m   \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_56777/4202895266.py\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(img_path, confidence)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0mpred_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_score\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_t\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_t\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "# https://haochen23.github.io/2020/04/object-detection-faster-rcnn.html#.Y0VLb8pBxJY\n",
    "\n",
    "FasterRCNNtest = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = FasterRCNNtest.roi_heads.box_predictor.cls_score.in_features\n",
    "FasterRCNNtest.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes=8)\n",
    "\n",
    "torch.save(FasterRCNNtest.state_dict(), f\"{'kitti-model'}/model-origin.pth\")\n",
    "FasterRCNNtest.load_state_dict(torch.load('./kitti-model/model4.pth'))\n",
    "\n",
    "FasterRCNNtest.eval()\n",
    "\n",
    "#FasterRCNN.eval()\n",
    "\n",
    "CLASS_NAMES = ['Car', 'Truck', 'Cyclist', 'Tram', 'Person_sitting', 'Misc', 'Van', 'Pedestrian']\n",
    "\n",
    "\n",
    "def get_prediction(img_path, confidence):\n",
    "  \"\"\"\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - confidence - threshold value for prediction score\n",
    "  \"\"\"\n",
    "  img = Image.open(img_path)\n",
    "  transform = T.Compose([T.ToTensor()])\n",
    "  img = transform(img)\n",
    "  pred = FasterRCNNtest([img]) # FasterRCNNtest\n",
    "  \n",
    "  print(pred)\n",
    "  if pred[0]['scores'].detach().numpy().size == 0:\n",
    "    return None, None\n",
    "  pred_class = [CLASS_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "  #//print(pred_score)\n",
    "  for x in pred_score:\n",
    "    if x > confidence:\n",
    "      print(pred_score.index(x))\n",
    "    if pred_score.index(x) == 0:\n",
    "      return [pred_boxes[0]], [pred_class[0]] \n",
    "    \n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x>confidence][-1]\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  return pred_boxes, pred_class\n",
    "\n",
    "\n",
    "def detect_object(img_path, confidence=0.5, rect_th=2, text_size=0.5, text_th=2):\n",
    "  \"\"\"\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - confidence - threshold value for prediction score\n",
    "      - rect_th - thickness of bounding box\n",
    "      - text_size - size of the class label text\n",
    "      - text_th - thichness of the text\n",
    "  \"\"\"\n",
    "  boxes, pred_cls = get_prediction(img_path, confidence)\n",
    "  if boxes == None and pred_cls == None:\n",
    "    print(\"NO Detection\")\n",
    "    return None\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "  for i in range(len(boxes)):\n",
    "    aa, bb = boxes[i][0][0], boxes[i][0][1]\n",
    "    a, b = boxes[i][1][0], boxes[i][1][1]\n",
    "    cv2.rectangle(img, (int(aa), int(bb)), (int(a), int(b)), \n",
    "                  color=(0, 255, 0), thickness=rect_th)\n",
    "    cv2.putText(img, pred_cls[i], (int(aa), int(bb)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                text_size, (0,255,0),thickness=text_th)\n",
    "  \n",
    "  plt.figure(figsize=(20,30))\n",
    "  plt.imshow(img)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()\n",
    "\n",
    "detect_object('./kitti-dataset/Kitti/raw/testing/image_2/000423.png', confidence=0.35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tflearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6188715f2b82f6c50c42d987f8df2d3ad6eac92c9a4f79f193fe0ffa7a2662c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
