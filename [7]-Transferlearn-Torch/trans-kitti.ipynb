{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete the Transfer Learn on KITTI with pytorch\n",
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/103862272\n",
    "# https://gauenk.github.io/docs/how_faster_rcnn_accepts_various_input_image_sizes.pdf\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]) \n",
    "\n",
    "# https://github.com/pytorch/vision/issues/215\n",
    "def target_transform(target):\n",
    "    pass\n",
    "\n",
    "# https://github.com/xiaowei305/perception/blob/9965418a43cc04dc8ca78e3b1324704501c257f0/ch2/densebox/train.py\n",
    "import numpy as np\n",
    "def kitti_collate_fn(batch_data):\n",
    "    class_names = ['Car', 'Truck', 'Cyclist', 'Tram', 'Person_sitting', 'Misc', 'Van', 'Pedestrian']\n",
    "    max_boxes = 11\n",
    "\n",
    "    unit_image, unit_target = [], []\n",
    "    for img, target in batch_data:\n",
    "        c, h, w = img.shape\n",
    "        boxes = np.array([x[\"bbox\"] for x in target if x['type'] != \"DontCare\"])\n",
    "        classes = np.array([class_names.index(x['type']) for x in target if x['type'] != \"DontCare\"])\n",
    "        #if len(boxes) <= max_boxes:\n",
    "        #    boxes = np.pad(boxes, ((0, max_boxes - len(boxes)), (0, 0))) # https://blog.csdn.net/Tan_HandSome/article/details/80296827\n",
    "        #    classes = np.pad(classes, (0, max_boxes - len(classes)))\n",
    "        #else:\n",
    "        #    boxes = boxes[:11]\n",
    "        #    classes = classes[:11]\n",
    "            \n",
    "        boxes /= np.array([w, h, w, h])\n",
    "        boxes = torch.from_numpy(np.stack(boxes, axis=0))\n",
    "        classes = torch.from_numpy(np.stack(classes, axis=0))\n",
    "\n",
    "        target_ = {}\n",
    "        target_['boxes'] = boxes\n",
    "        target_['labels'] = classes\n",
    "         \n",
    "        unit_image += [img]  \n",
    "        unit_target.append(target_) \n",
    "    return unit_image, unit_target\n",
    "\n",
    "# https://pytorch.org/vision/stable/generated/torchvision.datasets.Kitti.html#torchvision.datasets.Kitti\n",
    "# https://pytorch.org/vision/master/_modules/torchvision/datasets/kitti.html\n",
    "train_dataset = torchvision.datasets.Kitti(\n",
    "    root='./kitti-dataset',\n",
    "    train=True, download=True, transform=transform, #target_transform=target_transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=kitti_collate_fn)\n",
    "\n",
    "for batch in trainloader:\n",
    "    #print(batch)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Kitti\n",
      "    Number of datapoints: 401\n",
      "    Root location: ./kitti-dataset\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(800, 800), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Kitti\n",
      "    Number of datapoints: 201\n",
      "    Root location: ./kitti-dataset\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(800, 800), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_31375/4003097928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     test_dataset, batch_size=1, shuffle=True)\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#    pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.Kitti(\n",
    "    root='./kitti-dataset',\n",
    "    train=False, download=True, transform=transform\n",
    ")\n",
    "print(test_dataset)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# testloader just not fetch labels\n",
    "#for batch in testloader:\n",
    "#    print(batch)\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[0.5953, 0.4388, 0.2016, 0.0624],\n",
      "        [0.5888, 0.5681, 0.9583, 0.9076],\n",
      "        [0.9424, 0.0798, 0.0354, 0.2861],\n",
      "        [0.4587, 0.8284, 0.8997, 0.6154],\n",
      "        [0.7531, 0.8675, 0.5503, 0.1531],\n",
      "        [0.5151, 0.5926, 0.8113, 0.1086],\n",
      "        [0.0847, 0.2823, 0.6391, 0.6936],\n",
      "        [0.1617, 0.8279, 0.8987, 0.7467],\n",
      "        [0.0821, 0.2013, 0.0910, 0.4601],\n",
      "        [0.4938, 0.7785, 0.1791, 0.4087],\n",
      "        [0.6775, 0.8924, 0.9092, 0.0739]]), 'labels': tensor([76, 55, 68, 51, 63, 37, 35, 61, 42, 55, 68])}\n"
     ]
    }
   ],
   "source": [
    "# demo in pytorch document #!DONOT RUN THIS CELL\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "labels = torch.randint(1, 91, (4, 11)) # from 1 to 91, each box in each png match one label\n",
    "images = list(image for image in images)\n",
    "targets = []\n",
    "for i in range(len(images)):\n",
    "    d = {}\n",
    "    d['boxes'] = boxes[i]\n",
    "    d['labels'] = labels[i]\n",
    "    print(d)\n",
    "    break\n",
    "    targets.append(d)\n",
    "\n",
    "#len(images)  # 4\n",
    "#len(targets) # 4\n",
    "\n",
    "#output = model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/Applications/anaconda3/envs/tflearn/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFasterRCNN(\\n  (transform): GeneralizedRCNNTransform(\\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\\n  )\\n  (backbone): BackboneWithFPN(\\n    (body): IntermediateLayerGetter(\\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n      (relu): ReLU(inplace=True)\\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\\n      (layer1): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n            (1): FrozenBatchNorm2d(256, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n      (layer2): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n            (1): FrozenBatchNorm2d(512, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (3): Bottleneck(\\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n      (layer3): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (3): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (4): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (5): Bottleneck(\\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n      (layer4): Sequential(\\n        (0): Bottleneck(\\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n          (downsample): Sequential(\\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\\n          )\\n        )\\n        (1): Bottleneck(\\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n        (2): Bottleneck(\\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\\n          (relu): ReLU(inplace=True)\\n        )\\n      )\\n    )\\n    (fpn): FeaturePyramidNetwork(\\n      (inner_blocks): ModuleList(\\n        (0): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n        (1): Conv2dNormActivation(\\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n        (2): Conv2dNormActivation(\\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n        (3): Conv2dNormActivation(\\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\\n        )\\n      )\\n      (layer_blocks): ModuleList(\\n        (0): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n        (1): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n        (2): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n        (3): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n        )\\n      )\\n      (extra_blocks): LastLevelMaxPool()\\n    )\\n  )\\n  (rpn): RegionProposalNetwork(\\n    (anchor_generator): AnchorGenerator()\\n    (head): RPNHead(\\n      (conv): Sequential(\\n        (0): Conv2dNormActivation(\\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\\n          (1): ReLU(inplace=True)\\n        )\\n      )\\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\\n    )\\n  )\\n  (roi_heads): RoIHeads(\\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\\n    (box_head): TwoMLPHead(\\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\\n    )\\n    (box_predictor): FastRCNNPredictor(\\n      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\\n      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\\n    )\\n  )\\n)\\n\""
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://pytorch.org/vision/master/models/faster_rcnn.html\n",
    "FasterRCNN = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "'''\n",
    "FasterRCNN(\n",
    "  (transform): GeneralizedRCNNTransform(\n",
    "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
    "  )\n",
    "  (backbone): BackboneWithFPN(\n",
    "    (body): IntermediateLayerGetter(\n",
    "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "      (relu): ReLU(inplace=True)\n",
    "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "      (layer1): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
    "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (layer2): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (3): Bottleneck(\n",
    "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
    "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (layer3): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (3): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (4): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (5): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
    "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (layer4): Sequential(\n",
    "        (0): Bottleneck(\n",
    "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "          (downsample): Sequential(\n",
    "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          )\n",
    "        )\n",
    "        (1): Bottleneck(\n",
    "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "        (2): Bottleneck(\n",
    "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
    "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
    "          (relu): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (fpn): FeaturePyramidNetwork(\n",
    "      (inner_blocks): ModuleList(\n",
    "        (0): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): Conv2dNormActivation(\n",
    "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (2): Conv2dNormActivation(\n",
    "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (3): Conv2dNormActivation(\n",
    "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (layer_blocks): ModuleList(\n",
    "        (0): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "        (1): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "        (2): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "        (3): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (extra_blocks): LastLevelMaxPool()\n",
    "    )\n",
    "  )\n",
    "  (rpn): RegionProposalNetwork(\n",
    "    (anchor_generator): AnchorGenerator()\n",
    "    (head): RPNHead(\n",
    "      (conv): Sequential(\n",
    "        (0): Conv2dNormActivation(\n",
    "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (1): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
    "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
    "    )\n",
    "  )\n",
    "  (roi_heads): RoIHeads(\n",
    "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
    "    (box_head): TwoMLPHead(\n",
    "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
    "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "    )\n",
    "    (box_predictor): FastRCNNPredictor(\n",
    "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
    "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model pre-trained\n",
    "in_features = FasterRCNN.roi_heads.box_predictor.cls_score.in_features\n",
    "FasterRCNN.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for images, labels in iter(trainloader):\n",
    "    #print(images)\n",
    "    print(len(images))\n",
    "    #print(labels)\n",
    "    print(len(labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://debuggercafe.com/custom-object-detection-using-pytorch-faster-rcnn/\n",
    "- https://debuggercafe.com/a-simple-pipeline-to-train-pytorch-faster-rcnn-object-detection-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin\n",
    "NUM_EPOCHS = 3\n",
    "BEST_MODEL_PATH = 'kitti-model/best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "optimizer = optim.SGD(FasterRCNN.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    FasterRCNN.train()\n",
    "    for images, labels in iter(trainloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = FasterRCNN(images, labels)\n",
    "        #{'loss_classifier': tensor(3.0745, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0106, grad_fn=<DivBackward0>), 'loss_objectness': tensor(2.0545, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.3918, dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
    "        \n",
    "        #loss = F.cross_entropy(outputs, labels)\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #test_error_count = 0.0\n",
    "    #FasterRCNN.eval()\n",
    "    # NO testloader\n",
    "    #for images, targets in iter(trainloader):\n",
    "        #boxes = targets['boxes']\n",
    "        #labels = targets['labels']\n",
    "        \n",
    "        #pred_boxes, predicted_labels, predicted_scores = FasterRCNN(images)\n",
    "        \n",
    "        #test_error_count += float(torch.sum(torch.abs(labels - predicted_labels.argmax(1))))\n",
    "    \n",
    "    #test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset))\n",
    "    #print('%d: %f' % (epoch, test_accuracy))\n",
    "    #if test_accuracy > best_accuracy:\n",
    "    #    torch.save(FasterRCNN.state_dict(), BEST_MODEL_PATH)\n",
    "    #    best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 1:\n",
      "#### VALIDATING ####\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2163c038374cfdbdfebd9ff25a009e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### VALID END ####\n",
      "Epoch #0 train loss: 0.000\n",
      "Epoch #0 validation loss: 0.293\n",
      "SAVING MODEL COMPLETE...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BEST_MODEL_PATH = 'kitti-model/best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "        \n",
    "optimizer = optim.SGD(FasterRCNN.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "train_loss_list, val_loss_list = [], [] \n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "  \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}:\")\n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()\n",
    "    \n",
    "    '''\n",
    "    #FasterRCNN.train()\n",
    "    print(\"#### TRAINING ####\")\n",
    "    prog_bar = tqdm(trainloader, total=len(trainloader))\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        #//print(images, targets)\n",
    "        \n",
    "        loss_dict = FasterRCNN(images, targets)\n",
    "        #{'loss_classifier': tensor(3.0745, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0106, grad_fn=<DivBackward0>), 'loss_objectness': tensor(2.0545, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.3918, dtype=torch.float64, grad_fn=<DivBackward0>)}\n",
    "        #//print(loss_dict)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    print(\"#### TRAIN END ####\")\n",
    "    '''\n",
    "    \n",
    "    print(\"#### VALIDATING ####\")\n",
    "    prog_bar = tqdm(trainloader, total=len(trainloader))\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        \n",
    "        images, targets = data \n",
    "               \n",
    "        with torch.no_grad():\n",
    "            loss_dict = FasterRCNN(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        \n",
    "        val_itr += 1\n",
    "    print(\"#### VALID END ####\")\n",
    "    \n",
    "    print(f\"Epoch #{epoch} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value:.3f}\") \n",
    "    \n",
    "    if (epoch+1) % 1 == 0: # save model after every n epochs\n",
    "        torch.save(FasterRCNN.state_dict(), f\"{'kitti-model'}/model{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_31375/3272053547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mdetect_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./kitti-dataset/Kitti/raw/testing/image_2/000423.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_31375/3272053547.py\u001b[0m in \u001b[0;36mdetect_object\u001b[0;34m(img_path, confidence, rect_th, text_size, text_th)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;34m-\u001b[0m \u001b[0mtext_th\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthichness\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \"\"\"\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_31375/3272053547.py\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(img_path, confidence)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0mpred_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_score\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_t\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_t\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "# https://haochen23.github.io/2020/04/object-detection-faster-rcnn.html#.Y0VLb8pBxJY\n",
    "\n",
    "FasterRCNNtest = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = FasterRCNNtest.roi_heads.box_predictor.cls_score.in_features\n",
    "FasterRCNNtest.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes=8)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "FasterRCNNtest.load_state_dict(\n",
    "  torch.load('./kitti-model/model1.pth', map_location=device))\n",
    "FasterRCNNtest.eval()\n",
    "\n",
    "CLASS_NAMES = ['Car', 'Truck', 'Cyclist', 'Tram', 'Person_sitting', 'Misc', 'Van', 'Pedestrian']\n",
    "\n",
    "\n",
    "def get_prediction(img_path, confidence):\n",
    "  \"\"\"\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - confidence - threshold value for prediction score\n",
    "  \"\"\"\n",
    "  img = Image.open(img_path)\n",
    "  transform = T.Compose([T.ToTensor()])\n",
    "  img = transform(img)\n",
    "  pred = FasterRCNNtest([img])\n",
    "  print(pred)\n",
    "  pred_class = [CLASS_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "  print(pred_score)\n",
    "  for x in pred_score:\n",
    "    if x > confidence:\n",
    "      print(pred_score.index(x))\n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x>confidence][-1]\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  return pred_boxes, pred_class\n",
    "\n",
    "\n",
    "def detect_object(img_path, confidence=0.5, rect_th=2, text_size=2, text_th=2):\n",
    "  \"\"\"\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - confidence - threshold value for prediction score\n",
    "      - rect_th - thickness of bounding box\n",
    "      - text_size - size of the class label text\n",
    "      - text_th - thichness of the text\n",
    "  \"\"\"\n",
    "  boxes, pred_cls = get_prediction(img_path, confidence)\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "  for i in range(len(boxes)):\n",
    "    aa, bb = boxes[i][0][0], boxes[i][0][1]\n",
    "    a, b = boxes[i][1][0], boxes[i][1][1]\n",
    "    cv2.rectangle(img, (int(aa), int(bb)), (int(a), int(b)), \n",
    "                  color=(0, 255, 0), thickness=rect_th)\n",
    "    cv2.putText(img, pred_cls[i], (int(aa), int(bb)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                text_size, (0,255,0),thickness=text_th)\n",
    "  \n",
    "  plt.figure(figsize=(20,30))\n",
    "  plt.imshow(img)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()\n",
    "\n",
    "detect_object('./kitti-dataset/Kitti/raw/testing/image_2/000423.png', confidence=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tflearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6188715f2b82f6c50c42d987f8df2d3ad6eac92c9a4f79f193fe0ffa7a2662c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
